{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# <center> GEC\n",
    "GEC pour tester les modèles et les métriques en sortie.\n",
    "Implémentation de la métrique GLEU ?\n",
    "Utilisation du modèle BART ? Ou de l'implémentation de BERT ? Ou de l'implémentation de GPT-2 ? Ou de l'implémentation de T5 ? Ou de l'implémentation de GPT-3 ?\n",
    "\n",
    "Implémentation d'un modèle GEC en simulation de la pipeline.\n",
    "\n",
    "\n",
    "Modèle T5:\n",
    "https://huggingface.co/vennify/t5-base-grammar-correction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7600c773045bf8b0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Alors, de ce que j'ai vu on peut utiliser des modèles text to text comme le modèle transformer T5 de google ou le modèle BART de facebook.\n",
    "BART : https://ai.meta.com/research/publications/bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-translation-and-comprehension/\n",
    "T5 : https://blog.research.google/2020/02/exploring-transfer-learning-with-t5.html\n",
    "En se référant au SOTA de la tâche GEC : on peut considérer les modèles transformer comme étant les meilleurs modèles pour la tâche GEC.\n",
    "https://arxiv.org/pdf/2106.03830.pdf\n",
    "https://aclanthology.org/2022.coling-1.246/\n",
    "https://dl.acm.org/doi/abs/10.1145/3474840\n",
    "\n",
    "https://scholar.google.com/scholar?hl=fr&as_sdt=0%2C5&q=gec+grammatical+error+correction&btnG=&oq=GEC\n",
    "\n",
    "Aussi différents concours on eu lieu, comme le BEA :\n",
    "https://sig-edu.org/bea/2019/ en 2019 le BEA aborde la tâche GEC. https://www.cl.cam.ac.uk/research/nl/bea2019st/\n",
    "**https://competitions.codalab.org/competitions/20228#results**\n",
    "https://aclanthology.org/W19-4406.pdf\n",
    "\n",
    "SOTA 2020 : grammarly avec GECTor : https://arxiv.org/pdf/2005.12592v2.pdf   https://github.com/grammarly/gector\n",
    "best SOTA ? : https://aclanthology.org/2022.naacl-main.143/\n",
    "\n",
    "\n",
    "huggingface pretrained : https://huggingface.co/transformers/v3.3.1/pretrained_models.html\n",
    "\n",
    "https://huggingface.co/docs/transformers/model_doc/t5\n",
    "\n",
    "https://huggingface.co/docs/transformers/model_doc/bart\n",
    "papiers bart : https://arxiv.org/abs/2005.11849\n",
    "\n",
    "BTS TTT :  https://aclanthology.org/2021.wat-1.10.pdf\n",
    "\n",
    "https://aclanthology.org/W19-4413.pdf\n",
    "LLM GEC : https://aclanthology.org/2021.emnlp-main.611.pdf\n",
    "\n",
    "\n",
    "BERT\n",
    "https://sunilchomal.github.io/GECwBERT/\n",
    "https://huggingface.co/bert-base-uncased\n",
    "\n",
    "\n",
    "GPT3 https://aclanthology.org/2023.bea-1.18.pdf\n",
    "https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo\n",
    "https://arxiv.org/abs/2303.13648\n",
    "\n",
    "\n",
    "GEC VIDEO (BANGER0): https://aclanthology.org/2022.naacl-main.143.mp4"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbd5880dc014f9e5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "## Imports libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52d6dfdbe3f9505b"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch, os\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datetime import date\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from happytransformer import HappyTextToText, TTSettings"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T22:03:13.631308Z",
     "start_time": "2023-12-03T22:03:00.453528800Z"
    }
   },
   "id": "a042e2fcde0a16c5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "## Import des données"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb8a7173253b695a"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class ASR_Dataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.files = os.listdir(path)\n",
    "        self.sentences = []\n",
    "        for file in self.files:\n",
    "            with open(path + '\\\\' + file, 'r', encoding='utf-8') as file:\n",
    "                for line in file:\n",
    "                    self.sentences.append(line.strip())\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        transcript = self.sentences[idx]\n",
    "        return transcript\n",
    "\n",
    "\n",
    "def load_jfleg_dataset(path='data\\\\'):\n",
    "    jfleg_dataset = load_dataset(\"jfleg\", \"test\", split=\"test[:1%]\", cache_dir=path)\n",
    "    jfleg_Dataloader = DataLoader(jfleg_dataset, batch_size=1)\n",
    "    \n",
    "    print(\"Number of samples:\", len(jfleg_dataset))\n",
    "\n",
    "    sample_meta0 = jfleg_Dataloader.dataset[0]\n",
    "    print(\"\"\"\n",
    "    Transcript n°0 :\n",
    "\n",
    "    Path audio: {}\n",
    "    Sentence: {}\n",
    "    Corrections: {}\n",
    "    \"\"\".format(path, sample_meta0['sentence'], sample_meta0['corrections']))\n",
    "    \n",
    "    return jfleg_Dataloader\n",
    "\n",
    "def process_jfleg(data, model='T5'):\n",
    "    sentence = data['sentence']\n",
    "    corrections = data['corrections']\n",
    "    \n",
    "    return sentence[0], 0\n",
    "\n",
    "\n",
    "def load_ASR_dataset(path='data\\\\ASR', dataset=ASR_Dataset):\n",
    "    ASR_Dataset = dataset(path)\n",
    "    ASR_Dataloader = DataLoader(ASR_Dataset, batch_size=1)\n",
    "    \n",
    "    print(\"Number of samples:\", len(ASR_Dataset))\n",
    "    \n",
    "    sample_meta0 = ASR_Dataset\n",
    "    print(\"\"\"\n",
    "    Transcript n°0 :\n",
    "    \n",
    "    Transcript : {}\n",
    "    \"\"\".format(*sample_meta0))\n",
    "    \n",
    "    return ASR_Dataloader\n",
    "\n",
    "def process_ASR(data, model='T5'):\n",
    "    return data[0], 0\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T22:03:13.646708600Z",
     "start_time": "2023-12-03T22:03:13.637331Z"
    }
   },
   "id": "5edb74092353347e"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def GEC(model='T5', output_folder='output', dataset='jfleg'):\n",
    "    correction_folder = output_folder + \"/\" + model + '_' + dataset\n",
    "    correction_file =f'{correction_folder}/output_GEC.{date.today()}.txt'\n",
    "    os.makedirs(correction_folder, exist_ok=True)\n",
    "    \n",
    "    #TQDM loader\n",
    "    try:\n",
    "        function = \"load_\" + dataset + \"_dataset\"\n",
    "        dataloader = eval(function)()\n",
    "        dataloader_tqdm = tqdm(dataloader, total=len(dataloader))\n",
    "        \n",
    "        process_function = \"process_\" + dataset\n",
    "    except Exception as e:\n",
    "        raise ValueError(f'Unknown dataset {dataset}{e}')\n",
    "    \n",
    "    print(f'Generating corrections for {dataset} with {model} in {correction_folder}')\n",
    "    if model == 'T5':\n",
    "        T5 = HappyTextToText(\"T5\", \"vennify/t5-base-grammar-correction\")\n",
    "        args = TTSettings(num_beams=5, min_length=1)\n",
    "    \n",
    "    with open(correction_file, 'w', encoding='utf-8') as file:\n",
    "        file.write('')\n",
    " \n",
    "    for i, data in enumerate(dataloader_tqdm):\n",
    "        (sentence, out) = eval(process_function)(data, model=model)\n",
    "        if out == 1:\n",
    "            dataloader_tqdm.set_postfix({'status': 'Skipped', 'ID': i})\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            #Passage du transcript dans le modèle\n",
    "            if model == 'T5':\n",
    "                result = T5.generate_text(\"grammar:\" + sentence, args=args)\n",
    "                with open(correction_file, 'a', encoding='utf-8') as file:\n",
    "                    file.write(result.text + '\\n')\n",
    "            elif model == 'BART':\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(f'Unknown model {model}')\n",
    "            \n",
    "            dataloader_tqdm.set_postfix()\n",
    "\n",
    "        except Exception as e:\n",
    "            dataloader_tqdm.set_postfix({'status': 'Error', 'ID': i})\n",
    "            raise e \n",
    "        \n",
    "        # Validation and test with JFLEG\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T22:03:13.673605400Z",
     "start_time": "2023-12-03T22:03:13.652735200Z"
    }
   },
   "id": "45b5c48f45f4e44b"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 20\n",
      "\n",
      "    Transcript n°0 :\n",
      "    \n",
      "    Transcript : She don't like to eat vegetables.\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/20 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fc5f4ee6904842f384248b0db2a53747"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating corrections for ASR with T5 in output/T5_ASR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/03/2023 17:05:45 - INFO - happytransformer.happy_transformer -   Using device: cuda:0\n",
      "12/03/2023 17:05:45 - INFO - happytransformer.happy_transformer -   Moving model to cuda:0\n",
      "12/03/2023 17:05:45 - INFO - happytransformer.happy_transformer -   Initializing a pipeline\n",
      "D:\\Projets\\env\\3.10\\utils_env\\lib\\site-packages\\transformers\\pipelines\\base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "GEC(model='T5', output_folder='output', dataset='ASR')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T22:05:54.553174500Z",
     "start_time": "2023-12-03T22:05:42.726817300Z"
    }
   },
   "id": "f53a6e0cb62d5ebc"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 7\n",
      "\n",
      "    Transcript n°0 :\n",
      "\n",
      "    Path audio: data\\\n",
      "    Sentence: New and new technology has been introduced to the society .\n",
      "    Corrections: ['New technology has been introduced to society .', 'New technology has been introduced into the society .', 'Newer and newer technology has been introduced into society .', 'Newer and newer technology has been introduced to the society .']\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/7 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "77cbf07b10884854aad0fcd3c5b19e76"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating corrections for jfleg with T5 in output/T5_jfleg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/03/2023 17:03:24 - INFO - happytransformer.happy_transformer -   Using device: cuda:0\n",
      "12/03/2023 17:03:24 - INFO - happytransformer.happy_transformer -   Moving model to cuda:0\n",
      "12/03/2023 17:03:24 - INFO - happytransformer.happy_transformer -   Initializing a pipeline\n"
     ]
    }
   ],
   "source": [
    "GEC(model='T5', output_folder='output', dataset='jfleg')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T22:03:29.511821400Z",
     "start_time": "2023-12-03T22:03:21.662701800Z"
    }
   },
   "id": "514330ac0fc15077"
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "## Autres tests"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63092227cb1eca35"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text preprocessed: \n",
      " My name be Sylvain and I live in Paris, I to 25 year old.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Projets\\env\\3.10\\utils_env\\lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "GEC text: \n",
      " My name is Sylvain and I live in Paris. I am 25 years old.\n"
     ]
    }
   ],
   "source": [
    "# T5 test:\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
    "import torch\n",
    "#!pip3 install sentencepiece\n",
    "\n",
    "model_name = \"Unbabel/gec-t5_small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(\"cuda\")\n",
    "\n",
    "text = \"My name be Sylvain and I live in Paris, I to 25 year old.\"\n",
    "preprocess_text = text.strip().replace(\"\\n\",\"\")\n",
    "print (\"original text preprocessed: \\n\", preprocess_text)\n",
    "\n",
    "tokenized_text = tokenizer.encode(\"gec: \" + preprocess_text, return_tensors=\"pt\", max_length=128).to(\"cuda\")\n",
    "\n",
    "# summmarize\n",
    "summary_ids = model.generate(tokenized_text).to(\"cuda\")\n",
    "\n",
    "output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print (\"\\n\\nGEC text: \\n\",output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T22:03:31.461718200Z",
     "start_time": "2023-12-03T22:03:29.514822800Z"
    }
   },
   "id": "a5be8a12c9915b22"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Jfleg training:\n",
    "https://huggingface.co/vennify/t5-base-grammar-correction\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed3b228fa9399e7c"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n"
     ]
    }
   ],
   "source": [
    "model_b = T5ForConditionalGeneration.from_pretrained('t5-small').to(\"cuda\")\n",
    "stsb_sentence_1 = preprocess_text\n",
    "stsb_sentence_2 = output\n",
    "input_ids = tokenizer.encode(\"stsb sentence 1: \"+stsb_sentence_1+\" sentence 2: \"+stsb_sentence_2, return_tensors=\"pt\").to(\"cuda\")\n",
    "stsb_ids = model_b.generate(input_ids)\n",
    "stsb = tokenizer.decode(stsb_ids[0],skip_special_tokens=True)\n",
    "print(stsb)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T22:03:33.541295900Z",
     "start_time": "2023-12-03T22:03:31.450541Z"
    }
   },
   "id": "c05518eae2bd36be"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# <center> GEC\n",
    "GEC pour tester les modèles et les métriques en sortie.\n",
    "Implémentation de la métrique GLEU ?\n",
    "Utilisation du modèle BART ? Ou de l'implémentation de BERT ? Ou de l'implémentation de GPT-2 ? Ou de l'implémentation de T5 ? Ou de l'implémentation de GPT-3 ?\n",
    "\n",
    "Implémentation d'un modèle GEC en simulation de la pipeline.\n",
    "\n",
    "\n",
    "Modèle T5:\n",
    "https://huggingface.co/vennify/t5-base-grammar-correction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7600c773045bf8b0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Alors, de ce que j'ai vu on peut utiliser des modèles text to text comme le modèle transformer T5 de google ou le modèle BART de facebook.\n",
    "BART : https://ai.meta.com/research/publications/bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-translation-and-comprehension/\n",
    "T5 : https://blog.research.google/2020/02/exploring-transfer-learning-with-t5.html\n",
    "En se référant au SOTA de la tâche GEC : on peut considérer les modèles transformer comme étant les meilleurs modèles pour la tâche GEC.\n",
    "https://arxiv.org/pdf/2106.03830.pdf\n",
    "https://aclanthology.org/2022.coling-1.246/\n",
    "https://dl.acm.org/doi/abs/10.1145/3474840\n",
    "\n",
    "https://scholar.google.com/scholar?hl=fr&as_sdt=0%2C5&q=gec+grammatical+error+correction&btnG=&oq=GEC\n",
    "\n",
    "Aussi différents concours on eu lieu, comme le BEA :\n",
    "https://sig-edu.org/bea/2019/ en 2019 le BEA aborde la tâche GEC. https://www.cl.cam.ac.uk/research/nl/bea2019st/\n",
    "**https://competitions.codalab.org/competitions/20228#results**\n",
    "https://aclanthology.org/W19-4406.pdf\n",
    "\n",
    "SOTA 2020 : grammarly avec GECTor : https://arxiv.org/pdf/2005.12592v2.pdf   https://github.com/grammarly/gector\n",
    "best SOTA ? : https://aclanthology.org/2022.naacl-main.143/\n",
    "\n",
    "\n",
    "huggingface pretrained : https://huggingface.co/transformers/v3.3.1/pretrained_models.html\n",
    "\n",
    "https://huggingface.co/docs/transformers/model_doc/t5\n",
    "\n",
    "https://huggingface.co/docs/transformers/model_doc/bart\n",
    "papiers bart : https://arxiv.org/abs/2005.11849\n",
    "\n",
    "BTS TTT :  https://aclanthology.org/2021.wat-1.10.pdf\n",
    "\n",
    "https://aclanthology.org/W19-4413.pdf\n",
    "LLM GEC : https://aclanthology.org/2021.emnlp-main.611.pdf\n",
    "\n",
    "\n",
    "BERT\n",
    "https://sunilchomal.github.io/GECwBERT/\n",
    "https://huggingface.co/bert-base-uncased\n",
    "\n",
    "\n",
    "GPT3 https://aclanthology.org/2023.bea-1.18.pdf\n",
    "https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo\n",
    "https://arxiv.org/abs/2303.13648\n",
    "\n",
    "\n",
    "GEC VIDEO (BANGER0): https://aclanthology.org/2022.naacl-main.143.mp4"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbd5880dc014f9e5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "## Imports libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52d6dfdbe3f9505b"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch, os\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from happytransformer import HappyTextToText, TTSettings"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T20:44:46.362806200Z",
     "start_time": "2023-12-03T20:44:46.353122800Z"
    }
   },
   "id": "a042e2fcde0a16c5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "## Import des données"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb8a7173253b695a"
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "class ASR_Dataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.files = os.listdir(path)\n",
    "        self.sentences = []\n",
    "        for file in self.files:\n",
    "            with open(path + '\\\\' + file, 'r', encoding='utf-8') as file:\n",
    "                for line in file:\n",
    "                    self.sentences.append(line.strip())\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        transcript = self.sentences[idx]\n",
    "        return transcript\n",
    "\n",
    "\n",
    "def load_jfleg_dataset(path='data\\\\'):\n",
    "    jfleg_dataset = load_dataset(\"jfleg\", \"test\", split=\"test[:1%]\", cache_dir=path)\n",
    "    jfleg_Dataloader = DataLoader(jfleg_dataset, batch_size=1)\n",
    "    \n",
    "    print(\"Number of samples:\", len(jfleg_dataset))\n",
    "\n",
    "    sample_meta0 = jfleg_Dataloader.dataset[0]\n",
    "    print(\"\"\"\n",
    "    Transcript n°0 :\n",
    "\n",
    "    Path audio: {}\n",
    "    Sentence: {}\n",
    "    Corrections: {}\n",
    "    \"\"\".format(path, sample_meta0['sentence'], sample_meta0['corrections']))\n",
    "    \n",
    "    return jfleg_Dataloader\n",
    "\n",
    "def process_jfleg(data, model='T5'):\n",
    "    sentence = data['sentence']\n",
    "    corrections = data['corrections']\n",
    "    \n",
    "    return sentence[0], 0\n",
    "\n",
    "\n",
    "def load_ASR_dataset(path='data\\\\ASR', dataset=ASR_Dataset):\n",
    "    ASR_Dataset = dataset(path)\n",
    "    ASR_Dataloader = DataLoader(ASR_Dataset, batch_size=1)\n",
    "    \n",
    "    print(\"Number of samples:\", len(ASR_Dataset))\n",
    "    \n",
    "    sample_meta0 = ASR_Dataset\n",
    "    print(\"\"\"\n",
    "    Transcript n°0 :\n",
    "    \n",
    "    Transcript : {}\n",
    "    \"\"\".format(*sample_meta0))\n",
    "    \n",
    "    return ASR_Dataloader\n",
    "\n",
    "def process_ASR(data, model='T5'):\n",
    "    return data[0], 0\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T21:44:20.670197200Z",
     "start_time": "2023-12-03T21:44:20.654555Z"
    }
   },
   "id": "5edb74092353347e"
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "def GEC(model='T5', output_folder='output', dataset='jfleg'):\n",
    "    correction_folder = output_folder + \"/\" + model + '_' + dataset\n",
    "    os.makedirs(correction_folder, exist_ok=True)\n",
    "    \n",
    "    #TQDM loader\n",
    "    try:\n",
    "        function = \"load_\" + dataset + \"_dataset\"\n",
    "        dataloader = eval(function)()\n",
    "        dataloader_tqdm = tqdm(dataloader, total=len(dataloader))\n",
    "        \n",
    "        process_function = \"process_\" + dataset\n",
    "    except Exception as e:\n",
    "        raise ValueError(f'Unknown dataset {dataset}{e}')\n",
    "    \n",
    "    print(f'Generating corrections for {dataset} with {model} in {correction_folder}')\n",
    "    if model == 'T5':\n",
    "        T5 = HappyTextToText(\"T5\", \"vennify/t5-base-grammar-correction\")\n",
    "        args = TTSettings(num_beams=5, min_length=1)\n",
    "    \n",
    "    with open(correction_folder + '/output.txt', 'w', encoding='utf-8') as file:\n",
    "        file.write('')\n",
    " \n",
    "    for i, data in enumerate(dataloader_tqdm):\n",
    "        (sentence, out) = eval(process_function)(data, model=model)\n",
    "        if out == 1:\n",
    "            dataloader_tqdm.set_postfix({'status': 'Skipped', 'ID': i})\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            #Passage du transcript dans le modèle\n",
    "            if model == 'T5':\n",
    "                result = T5.generate_text(\"grammar:\" + sentence, args=args)\n",
    "                with open(correction_folder + '/output.txt', 'a', encoding='utf-8') as file:\n",
    "                    file.write(result.text + '\\n')\n",
    "            elif model == 'BART':\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(f'Unknown model {model}')\n",
    "            \n",
    "            dataloader_tqdm.set_postfix()\n",
    "\n",
    "        except Exception as e:\n",
    "            dataloader_tqdm.set_postfix({'status': 'Error', 'ID': i})\n",
    "            raise e \n",
    "        \n",
    "        # Validation and test with JFLEG\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T21:44:20.909616800Z",
     "start_time": "2023-12-03T21:44:20.891042800Z"
    }
   },
   "id": "45b5c48f45f4e44b"
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10\n",
      "\n",
      "    Transcript n°0 :\n",
      "    \n",
      "    Transcript : She don't like to eat vegetables.\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "16e9cf3b3f2d4cf094fd5b8a49faeabe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating corrections for ASR with T5 in output/T5_ASR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/03/2023 16:43:46 - INFO - happytransformer.happy_transformer -   Using device: cuda:0\n",
      "12/03/2023 16:43:46 - INFO - happytransformer.happy_transformer -   Moving model to cuda:0\n",
      "12/03/2023 16:43:47 - INFO - happytransformer.happy_transformer -   Initializing a pipeline\n"
     ]
    }
   ],
   "source": [
    "GEC(model='T5', output_folder='output', dataset='ASR')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T21:43:59.387262200Z",
     "start_time": "2023-12-03T21:43:43.993530700Z"
    }
   },
   "id": "f53a6e0cb62d5ebc"
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 7\n",
      "\n",
      "    Transcript n°0 :\n",
      "\n",
      "    Path audio: data\\\n",
      "    Sentence: New and new technology has been introduced to the society .\n",
      "    Corrections: ['New technology has been introduced to society .', 'New technology has been introduced into the society .', 'Newer and newer technology has been introduced into society .', 'Newer and newer technology has been introduced to the society .']\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/7 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c0ecae617f5d45a5af446925d197c9a2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating corrections for jfleg with T5 in output/T5_jfleg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/03/2023 16:44:26 - INFO - happytransformer.happy_transformer -   Using device: cuda:0\n",
      "12/03/2023 16:44:26 - INFO - happytransformer.happy_transformer -   Moving model to cuda:0\n",
      "12/03/2023 16:44:27 - INFO - happytransformer.happy_transformer -   Initializing a pipeline\n"
     ]
    }
   ],
   "source": [
    "GEC(model='T5', output_folder='output', dataset='jfleg')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T21:44:43.302271900Z",
     "start_time": "2023-12-03T21:44:23.461156100Z"
    }
   },
   "id": "514330ac0fc15077"
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "## Autres tests"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63092227cb1eca35"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text preprocessed: \n",
      " My name be Sylvain and I live in Paris, I to 25 year old.\n",
      "\n",
      "\n",
      "GEC text: \n",
      " My name is Sylvain and I live in Paris. I am 25 years old.\n"
     ]
    }
   ],
   "source": [
    "# T5 test:\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
    "import torch\n",
    "#!pip3 install sentencepiece\n",
    "\n",
    "model_name = \"Unbabel/gec-t5_small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(\"cuda\")\n",
    "\n",
    "text = \"My name be Sylvain and I live in Paris, I to 25 year old.\"\n",
    "preprocess_text = text.strip().replace(\"\\n\",\"\")\n",
    "print (\"original text preprocessed: \\n\", preprocess_text)\n",
    "\n",
    "tokenized_text = tokenizer.encode(\"gec: \" + preprocess_text, return_tensors=\"pt\", max_length=128).to(\"cuda\")\n",
    "\n",
    "# summmarize\n",
    "summary_ids = model.generate(tokenized_text).to(\"cuda\")\n",
    "\n",
    "output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print (\"\\n\\nGEC text: \\n\",output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T20:06:27.108614300Z",
     "start_time": "2023-12-03T20:06:25.369472600Z"
    }
   },
   "id": "a5be8a12c9915b22"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Jfleg training:\n",
    "https://huggingface.co/vennify/t5-base-grammar-correction\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed3b228fa9399e7c"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n"
     ]
    }
   ],
   "source": [
    "model_b = T5ForConditionalGeneration.from_pretrained('t5-small').to(\"cuda\")\n",
    "stsb_sentence_1 = preprocess_text\n",
    "stsb_sentence_2 = output\n",
    "input_ids = tokenizer.encode(\"stsb sentence 1: \"+stsb_sentence_1+\" sentence 2: \"+stsb_sentence_2, return_tensors=\"pt\").to(\"cuda\")\n",
    "stsb_ids = model_b.generate(input_ids)\n",
    "stsb = tokenizer.decode(stsb_ids[0],skip_special_tokens=True)\n",
    "print(stsb)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T20:04:34.599384400Z",
     "start_time": "2023-12-03T20:04:33.420365200Z"
    }
   },
   "id": "c05518eae2bd36be"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

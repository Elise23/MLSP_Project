{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# <center> TEXT TO SPEECH </center>\n",
    "\n",
    "Notebook de test et d'implémentation de modèles de TTS."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9e6e100228ae717"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modèles\n",
    "\n",
    "On va premièrement utiliser le modèle [gTTS](https://gtts.readthedocs.io/en/latest/) qui est un modèle de TTS (Text To Speech) basé sur Google Translate. Celui-ci est très simple à utiliser, mais ne permet pas de modifier beaucoup de paramètres du modèle.\n",
    "\n",
    "Modèles testés:\n",
    "- [gTTS](https://gtts.readthedocs.io/en/latest/)\n",
    "- [Tacotron2](https://pytorch.org/hub/nvidia_deeplearningexamples_tacotron2/)\n",
    "\n",
    "Autres modèles de TTS :\n",
    "- [Tacotron2](https://huggingface.co/transformers/model_doc/tacotron2.html) (exemple avec vocodeur HiFiGAN, sur modèle pré-entrainé avec le dataset LJSpeech [ici](https://huggingface.co/speechbrain/tts-tacotron2-ljspeech))\n",
    "- [FastSpeech2](https://fastspeech2.github.io/fastspeech2/)\n",
    "Vocodeurs:\n",
    "- [HiFiGAN](https://pytorch.org/hub/nvidia_deeplearningexamples_hifigan/)\n",
    "- [MelGAN](https://paperswithcode.com/method/melgan)\n",
    "- WaveGlow\n",
    "- WaveRNN\n",
    "\n",
    "Modèle utilisé en pipeline : [gTTS](https://gtts.readthedocs.io/en/latest/)\n",
    "\n",
    "Le modèle TacoTron2 est un modèle de TTS basé sur un encodeur-décodeur. Il prend en entrée un texte et génère un spectrogramme. Ce spectrogramme est ensuite passé dans un vocodeur qui génère le fichier audio. Grâce au spectrogramme généré, on peut espérer pouvoir évaluer la qualité du modèle en comparant les spectrogrammes de nos audio d'évaluation et ceux générés par le vocodeur."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3671d192b6d1efa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation\n",
    "\n",
    "Il est très difficile d'évaluer la qualité d'un modèle de TTS. En effet, il n'existe pas de métrique permettant de quantifier la qualité d'un fichier audio. On peut cependant utiliser des métriques de similarité entre spectrogrammes, ou encore des métriques de similarité entre les fichiers audios. Cependant, ces métriques ne sont pas forcément représentatives de la qualité du fichier audio.\n",
    "\n",
    "Dans notre cas nous allons principalement nous reposer sur le MOS (Mean Opinion Score) qui est une métrique subjective. Celle-ci est obtenue en faisant écouter un fichier audio à un panel de personnes, et en leur demandant de noter la qualité du fichier audio entre 1 et 5. On peut ensuite calculer la moyenne des notes obtenues pour obtenir le MOS.\n",
    "\n",
    "Nous allons utiliser le dataset [LibriSpeech](https://huggingface.co/datasets/librispeech_asr) qui est un dataset de fichiers audios de livres audio. Ce dataset est composé de 1000h d'audio environ. Chaque fichier audio est accompagné d'un transcript. Nous allons utiliser ce transcript pour générer un fichier audio avec notre modèle de TTS, et ensuite comparer le fichier audio généré avec le fichier audio original.\n",
    "\n",
    "Comme dit précédemment, nous allons aussi essayer d'évaluer les audio en comparant les spectrogrammes (mel cepstral distortion). Cela peut se faire en utilisant Tacotron2 qui génère un spectrogramme à partir d'un texte de jfleg, et en comparant ce spectrogramme avec celui récupéré de l'audio jgelf correspondant au texte."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0cc7d8aa9cfed96"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import des librairies"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f008b45fc82f23"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "import torchaudio, torch, os\n",
    "import IPython.display as ipd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from gtts import gTTS\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "!pip3 install -q -U deep_phonemizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T05:33:11.022886700Z",
     "start_time": "2023-12-13T05:33:06.565767200Z"
    }
   },
   "id": "30ea638c53648590"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Définition des fonctions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e40f713ac9c22e9b"
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [],
   "source": [
    "class GEC_Dataset(Dataset):\n",
    "    def __init__(self, path, text):\n",
    "        \"\"\"\n",
    "        Dataset pour les données en sortie du GEC\n",
    "        :param path: Chemin des données\n",
    "        :param text: Texte corrigé (Default None, on utilise les données du path. Pour une utilisation du TTS en pipeline alors text est le texte corrigé)\n",
    "        \"\"\"\n",
    "        self.sentences = []\n",
    "        \n",
    "        if text is not None:\n",
    "            self.sentences.append(text.strip())\n",
    "        else:\n",
    "            self.path = path\n",
    "            self.files = os.listdir(path)\n",
    "            for file in self.files:\n",
    "                with open(path + '\\\\' + file, 'r', encoding='utf-8') as file:\n",
    "                    for line in file:\n",
    "                        self.sentences.append(line.strip())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        transcript = self.sentences[idx]\n",
    "\n",
    "        return transcript, idx\n",
    "\n",
    "\n",
    "def load_librispeech_dataset(path='data\\\\'):\n",
    "    \"\"\"\n",
    "    Fonction permettant de charger le dataset LibriSpeech\n",
    "    :param path: Chemin du dataset\n",
    "    :return: Dataloader du dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    librispeech_dataset = torchaudio.datasets.LIBRISPEECH(path, url='dev-clean', download=True)\n",
    "    #Shorten dataset to 500 samples\n",
    "    librispeech_dataset_short = torch.utils.data.Subset(librispeech_dataset, range(1))\n",
    "    librispeech_dataloader = DataLoader(librispeech_dataset_short, batch_size=1)\n",
    "    \n",
    "    print(\"Number of samples:\", len(librispeech_dataset_short))\n",
    "    \n",
    "    sample_meta0 = librispeech_dataset.get_metadata(0)\n",
    "    sample_0 = librispeech_dataset_short[0][0]\n",
    "    print(\"\"\"\n",
    "    Audio n°0 :\n",
    "    \n",
    "    Path audio: {}{}\n",
    "    Sample rate : {}\n",
    "    Transcript : {}\n",
    "    Speaker ID : {}\n",
    "    Chapter ID : {}\n",
    "    Utterance ID : {}\n",
    "    \"\"\".format(path + \"LibriSpeech\", *sample_meta0))\n",
    "    \n",
    "    #Playing sample_O\n",
    "    ipd.Audio(sample_0, rate=sample_meta0[1])\n",
    "    \n",
    "    return librispeech_dataloader\n",
    "\n",
    "def process_librispeech(data, model='gtts', output_folder='output'):\n",
    "    (waveform, sample_rate, transcript, ID_s, ID_c, ID_u) = data\n",
    "    \n",
    "    waveform = waveform.squeeze(0)\n",
    "    sample_rate = sample_rate.squeeze(0)\n",
    "    transcript = str(transcript[0])\n",
    "    ID = \"-\".join((str(ID_s.item()), str(ID_c.item()), str(ID_u.item()))).replace(',','')\n",
    "    \n",
    "    file = f'./{output_folder}/out_{ID}.mp3'\n",
    "    \n",
    "    if os.path.exists(file):\n",
    "        return ID, file, 1, None, None\n",
    "    \n",
    "    return ID, file, 0, transcript, (waveform, sample_rate)\n",
    "\n",
    "\n",
    "def load_GEC_dataset(path='data\\\\GEC', text=None, dataset=GEC_Dataset):\n",
    "    \"\"\"\n",
    "    Fonction permettant de charger le dataset GEC\n",
    "    :param path: Chemin du dataset\n",
    "    :param text: Texte corrigé (Default None, on utilise les données du path. Pour une utilisation du TTS en pipeline alors text est le texte corrigé)\n",
    "    :param dataset: Classe du dataset à utiliser (Default GEC_Dataset)\n",
    "    :return: Dataloader du dataset\n",
    "    \"\"\"\n",
    "    GEC_Dataset = dataset(path, text)\n",
    "    GEC_Dataloader = DataLoader(GEC_Dataset, batch_size=1)\n",
    "    \n",
    "    print(\"Number of samples:\", len(GEC_Dataset))\n",
    "    \n",
    "    sample_meta0 = GEC_Dataloader.dataset[0]\n",
    "    print(\"\"\"\n",
    "    Transcript n°0 :\n",
    "    \n",
    "    Transcript : {}\n",
    "    Transcript ID : {}\n",
    "    \"\"\".format(*sample_meta0))\n",
    "    \n",
    "    return GEC_Dataloader\n",
    "\n",
    "\n",
    "def process_GEC(data, model='gtts', output_folder='output'):\n",
    "    (transcript, ID) = data\n",
    "    ID = ID.item()\n",
    "    file = f'./{output_folder}/out_{ID}.mp3'\n",
    "    \n",
    "    return ID, file, 0, transcript, None\n",
    "\n",
    "def plot(waveforms, spec, sample_rate):\n",
    "    waveforms = waveforms.cpu().detach()\n",
    "\n",
    "    fig, [ax1, ax2] = plt.subplots(2, 1)\n",
    "    ax1.plot(waveforms[0])\n",
    "    ax1.set_xlim(0, waveforms.size(-1))\n",
    "    ax1.grid(True)\n",
    "    ax2.imshow(spec[0].cpu().detach(), origin=\"lower\", aspect=\"auto\")\n",
    "    return IPython.display.Audio(waveforms[0:1], rate=sample_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T06:14:43.498620600Z",
     "start_time": "2023-12-13T06:14:43.468055600Z"
    }
   },
   "id": "6f03872312cff1f6"
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "outputs": [],
   "source": [
    "def TTS(text:str=None, model='gtts', output_folder='output', dataset='GEC'):\n",
    "    \"\"\"\n",
    "    Fonction permettant de générer des fichiers audios à partir d'un texte\n",
    "    :param text: Texte à transformer en audio (Default None, on utilise les données du dataset. Pour une utilisation du TTS en pipeline alors text est le texte corrigé)\n",
    "    :param model: Modèle à utiliser (Default gtts)\n",
    "    :param output_folder: Dossier de sortie (Default output)\n",
    "    :param dataset: Dataset à utiliser (Default GEC)\n",
    "    :return: Fichier audio généré (waveform, sample_rate)\n",
    "    \"\"\"\n",
    "    \n",
    "    pipeline = \"/pipeline/\" if text is not None else \"/\"\n",
    "    output_folder = output_folder + pipeline + model + '_' + dataset\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    # TQDM loader\n",
    "    try:\n",
    "        function = \"load_\" + dataset + \"_dataset\"\n",
    "        dataloader = eval(function)(text=text)\n",
    "        dataloader_tqdm = tqdm(dataloader, total=len(dataloader))\n",
    "    \n",
    "        process_function = \"process_\" + dataset\n",
    "        \n",
    "    except NameError:\n",
    "        raise ValueError(f'Unknown dataset {dataset}')\n",
    "    \n",
    "    bundle = torchaudio.pipelines.TACOTRON2_WAVERNN_PHONE_LJSPEECH\n",
    "    processor = bundle.get_text_processor()\n",
    "    tacotron2 = bundle.get_tacotron2().to(\"cuda\")\n",
    "    file_out=''\n",
    "    print(f'Generating audio files with {model} model in {output_folder} folder')\n",
    "    \n",
    "    for i, data in enumerate(dataloader_tqdm):\n",
    "        (ID, file_out, out, transcript, evalua) = eval(process_function)(data, model=model, output_folder=output_folder)\n",
    "        file_out = file_out if type(file_out) == str else file_out[0]\n",
    "        transcript = str(transcript)\n",
    "        if out == 1:\n",
    "            dataloader_tqdm.set_postfix({'status': 'Skipped', 'ID': ID})\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            #Passage du transcript dans le modèle\n",
    "            if model == 'gtts':\n",
    "                tts = gTTS(transcript, lang='en')#, tld='co.in')\n",
    "                tts.save(file_out)\n",
    "            elif model == 'tacotron2':\n",
    "                with torch.inference_mode():\n",
    "                    processed, lengths = processor(transcript)\n",
    "                    processed = processed.to(\"cuda\")\n",
    "                    lengths = lengths.to(\"cuda\")\n",
    "                    spec, spec_l, _ = tacotron2.infer(processed, lengths)\n",
    "\n",
    "                plt.imshow(spec[0].cpu().detach(), origin=\"lower\", aspect=\"auto\", cmap=\"viridis\")\n",
    "                plt.show()\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f'Unknown model {model}')\n",
    "            \n",
    "            dataloader_tqdm.set_postfix()\n",
    "        \n",
    "\n",
    "        except Exception as e:\n",
    "            dataloader_tqdm.set_postfix({'status': 'Error', 'ID': ID})\n",
    "            raise e\n",
    "            \n",
    "        \n",
    "        if evalua != None:\n",
    "            waveform, sample_rate = evalua\n",
    "            transform = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, n_fft=1024, win_length=1024, hop_length=256)\n",
    "            mel_specgram = transform(waveform)\n",
    "            mag_spect = torch.stft(waveform, 1024, window=torch.hamming_window(1024), return_complex=True, hop_length=256)\n",
    "            mag_spect = torch.sqrt(torch.abs(mag_spect.permute(1,2,0)))\n",
    "            mel_specgram = torch.sqrt(torch.abs(mel_specgram))\n",
    "            plt.imshow(mel_specgram[0], origin=\"lower\", aspect=\"auto\", cmap=\"viridis\")\n",
    "            from sklearn.metrics.pairwise import cosine_similarity\n",
    "            import numpy as np\n",
    "\n",
    "                # Flatten the spectrograms\n",
    "            spec1 = mel_specgram[0].cpu().detach()\n",
    "            spec2 = spec.squeeze().cpu().detach()\n",
    "            \n",
    "            \n",
    "            shape1 = spec1.shape\n",
    "            shape2 = spec2.shape\n",
    "            \n",
    "            # Find the size differences along both axes\n",
    "            size_diff_rows = shape2[0] - shape1[0]\n",
    "            size_diff_cols = shape2[1] - shape1[1]\n",
    "            \n",
    "            if size_diff_rows > 0:      \n",
    "                padded_spec1 = np.pad(spec1, ((0, size_diff_rows), (0, 0)), mode='constant')\n",
    "            else:\n",
    "                padded_spec1 = spec1\n",
    "            \n",
    "            if size_diff_cols > 0:\n",
    "                padded_spec2 = np.pad(spec2, ((0, 0), (0, size_diff_cols)), mode='constant')\n",
    "            else:\n",
    "                padded_spec2 = spec2\n",
    "\n",
    "\n",
    "            flat_spec1 = padded_spec1.flatten()\n",
    "            flat_spec2 = padded_spec2.flatten()\n",
    "            min_size = min(len(flat_spec1), len(flat_spec2))\n",
    "            flat_spec1 = flat_spec1[:min_size]\n",
    "            flat_spec2 = flat_spec2[:min_size]\n",
    "            # Compute cosine similarity\n",
    "            similarity_measure = cosine_similarity([flat_spec1], [flat_spec2])[0, 0]\n",
    "            print(similarity_measure)\n",
    "            \n",
    "            mse = np.mean((padded_spec1 - padded_spec2)**2)\n",
    "            similarity_measure = 1 / (1 + mse) \n",
    "            print(similarity_measure)\n",
    "            \n",
    "            from fastdtw import fastdtw\n",
    "\n",
    "            # Assuming spec1 and spec2 are the normalized spectrograms\n",
    "            distance, path = fastdtw(padded_spec1, padded_spec2)\n",
    "            similarity_measure = 1 / (1 + distance)  # Invert to get a similarity measure\n",
    "            print(similarity_measure)\n",
    "    \n",
    "    \n",
    "    return torchaudio.load(file_out) if file_out != '' else None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T06:59:33.477015600Z",
     "start_time": "2023-12-13T06:59:33.451003600Z"
    }
   },
   "id": "ad05bc0ee24afe69"
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "## TTS des modèles et des datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6336b7c6ec1a2f7"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TTS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mTTS\u001B[49m(model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtacotron2\u001B[39m\u001B[38;5;124m'\u001B[39m, output_folder\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moutput\u001B[39m\u001B[38;5;124m'\u001B[39m, dataset\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlibrispeech\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'TTS' is not defined"
     ]
    }
   ],
   "source": [
    "# Tacotron2 avec dataset LibriSpeech\n",
    "TTS(model='tacotron2', output_folder='output', dataset='librispeech')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T21:40:14.259676Z",
     "start_time": "2023-12-19T21:40:13.961745400Z"
    }
   },
   "id": "16610c327e11f0a7"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 17\n",
      "\n",
      "    Transcript n°0 :\n",
      "    \n",
      "    Transcript : She doesn't like to eat vegetables.\n",
      "    Transcript ID : 0\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/17 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "68e4b086ae79447cb3f7086a6b0da7bb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating audio files with gtts model in output/gtts_GEC folder\n"
     ]
    }
   ],
   "source": [
    "# gTTS avec dataset GEC\n",
    "TTS(model=\"gtts\", output_folder=\"output\", dataset=\"GEC\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T22:18:12.936185800Z",
     "start_time": "2023-12-03T22:18:05.889594700Z"
    }
   },
   "id": "7b4eb5a265d99e4c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
